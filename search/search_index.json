{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"WEB CRAWLING Identitas : Nurul Vicky Wahdaniah 160411100128 Mata Kuliah Penambangan dan Pencarian Web Pendahuluan Halaman ini berisi tentang langkah - langkah crawling data dari sebuah website, kemudian dilanjutkan dengan pre-processing, seleksi fitur hingga clustering data. Tools and Requirements Website URL : https://www.malasngoding.com/ Python 3.6 Python Libraries : BeautifulSoup4 Berfungsi untuk mengcrawl data pada website. Cara menginstall BeautifulSoup4 menggunakan pip : pip install beautifulsoup4 SQLite3 Berfungsi sebagai Database untuk penyimpanan data dari website. CSV Berfungsi untuk read and write tabular data dalam format CSV. Cara menginstall CSV module menggunakan pip : pip install python-csv Sastrawi Berfungsi untuk steeming pada bagian pre-processing data. Steeming bertujuan untuk mentransformasikan kata menjadi kata dasar tanpa imbuhan, baik awalan, akhiran atau sisipan. Casa menginstall Sastrawi menggunakan pip : pip install Sastrawi Math Berfungsi pada proses-proses yang menggunakan fungsi matematika. Cara menginstall modul Math menggunakan pip : pip install math Numpy Berfungsi untuk operasi vektor dan matriks untuk keperluan analisis data. Cara menginstall Numpy menggunakan pip : pip install numpy Sckit-Learn Berfungsi memberikan sejumlah fitur untuk keperluan data science seperti Algoritma Regresi, Algoritma Naive Bayes, Algoritma Clustering, Algoritma Decision Tree, Data Prepocessing Tool, dan lain - lain. Cara menginstall Sckit-Learn menggunakan pip : pip install scikit-learn Sckit-Fuzzy Merupakan kumpulan dari algoritma fuzzy yang digunakan pada modul SciPy atau SkLearn. Cara menginstall Sckit-Fuzzy menggunakan pip : pip install scikit-fuzzy Database KBI Digunakan pada proses pre-processing untuk memisahkan kata penting pada data. Crawling Website URL : https://www.malasngoding.com/ Data yang diambil dari website di atas berupa data Text, yaitu Judul Artikel dan Isi Artikel. Langkah Proses Crawling : import library yang akan digunakan untuk crawl data. from bs4 import BeautifulSoup Request pada halaman web tertuju. src = \"https://www.malasngoding.com/\" page = requests.get(src) soup = BeautifulSoup(page.content, 'html.parser') simpan elemen/data (judul dan isi) yang akan di crawl pada variabel berdasar class pada tag html web tertuju. linkhead = soup.findAll(class_='text-dark') pagination = soup.find(class_='next page-numbers') code di atas merupakan variabel penampung untuk link yang menuju judul dan isi artikel konten = soup.find('article') title = konten.find(class_='post-title entry-title pb-2').getText() temp = konten.findAll('p') code di atas merupakan variabel yang menampung judul dan isi artikel Masukkan data ke dalam Database. conn = sqlite3.connect('articles.sqlite') conn.execute('''CREATE TABLE if not exists ARTICLES (TITLE TEXT NOT NULL, ISI TEXT NOT NULL);''') conn.execute(\"INSERT INTO ARTICLES (TITLE, ISI) VALUES (?, ?)\", (title, isif)); conn.commit() export data ke dalam format CSV ``` def write_csv(nama_file, isi, tipe='w'): 'tipe=w; write; tipe=a; append;' with open(nama_file, mode=tipe) as tbl: tbl_writer = csv.writer(tbl, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL) for row in isi: tbl_writer.writerow(row) ``` Kode Program Lengkap Proses Crawling : conn = sqlite3.connect('articles.sqlite') conn.execute('''CREATE TABLE if not exists ARTICLES (TITLE TEXT NOT NULL, ISI TEXT NOT NULL);''') conn.commit() src = \"https://www.malasngoding.com/\" n = 1 while n <= 2: print(\"page \",n) page = requests.get(src) soup = BeautifulSoup(page.content, 'html.parser') linkhead = soup.findAll(class_='text-dark') pagination = soup.find(class_='next page-numbers') for links in linkhead: try : src = links['href'] page = requests.get(src) soup = BeautifulSoup(page.content, 'html.parser') konten = soup.find('article') title = konten.find(class_='post-title entry-title pb-2').getText() temp = konten.findAll('p') isi = [] for j in range(len(temp)): isi += [temp[j].getText()] isif = \"\" for i in isi: isif += i conn.execute(\"INSERT INTO ARTICLES (TITLE, ISI) VALUES (?, ?)\", (title, isif)); except AttributeError: continue conn.commit() src = pagination['href'] n+=1 def write_csv(nama_file, isi, tipe='w'): 'tipe=w; write; tipe=a; append;' with open(nama_file, mode=tipe) as tbl: tbl_writer = csv.writer(tbl, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL) for row in isi: tbl_writer.writerow(row) Pre-Processing Pre-processing merupakan tahapan dimana program melakukan seleksi data yang akan di proses. Proses Pre-Processing meliputi : Case Folding Case Folding dibutuhkan dalam mengkonversi keseluruhan text dalam dokumen menjadi suatu bentuk standar/lower case. Lebih ringkasnya, case folding mengubah semua huruf dalam dokumen menjadi huruf kecil. hara huruf 'a' sampai 'z' yang diterima. Karakter selain huruf akan dihilangkan. Tokenizing Tahap tokenizing merupakan tahap pemecahan kalimat menjadi beberapa kata tunggal. Filtering (Stopword) Tahap filtering adalah tahap mengambil kata-kata penting dari hasil token. Stopword adalah kata-kata yang tidak deskriptif yang dapat dibuang dalam pendekatan bag-of-words. Stemming Sremming diperlukan untuk memperkecil jumlah indeks yang berbeda dari suatu dokumen, juga untuk melakukan pengelompokan kata-kata lain yang memiliki kata dasar dan arti yang serupa namun memiliki bentuk atau form yang berbeda karena mendapat imbuhan yang berbeda. Kode Program Pre-Processing cursor = conn.execute(\"SELECT* from ARTICLES\") isif ='' for row in cursor: isif+=row[1] factory = StopWordRemoverFactory() stopword = factory.create_stop_word_remover() from Sastrawi.Stemmer.StemmerFactory import StemmerFactory factory = StemmerFactory() stemmer = factory.create_stemmer() stop = stopword.remove(isif) stem = stemmer.stem(stop) katadasar = stem.split() matrix=[] for row in cursor: tampung = [] for i in katadasar: tampung.append(row[1].lower().count(i)) matrix.append(tampung) write_csv(\"kata_before_%s.csv\"%n, katadasar) conn = sqlite3.connect('KBI.db') cur_kbi = conn.execute(\"SELECT* from KATA\") def LinearSearch (kbi,kata): found=False posisi=0 while posisi < len (kata) and not found : if kata[posisi]==kbi: found=True posisi=posisi+1 return found berhasil=[] berhasil2='' for kata in cur_kbi : ketemu=LinearSearch(kata[0],katadasar) if ketemu : kata = kata[0] berhasil.append(kata) berhasil2=berhasil2+' '+kata TF - IDF TF (Term Frequency) TF (Term Frequency) adalah frekuensi dari kemunculan sebuah term dalam dokumen yang bersangkutan. Semakin besar jumlah kemunculan suatu term (TF tinggi) dalam dokumen, semakin besar pula bobotnya atau akan memberikan nilai kesesuaian yang semakin besar. IDF (Inverse Document Frequency) IDF (Inverse Document Frequency) merupakan sebuah perhitungan dari bagaimana term didistribusikan secara luas pada koleksi dokumen yang bersangkutan. IDF menunjukkan hubungan ketersediaan sebuah term dalam seluruh dokumen. Semakin sedikit jumlah dokumen yang mengandung term yang dimaksud, maka nilai IDF semakin besar. Kode Program TF-IDF #TF df = list() for d in range (len(matrix2[0])): total = 0 for i in range(len(matrix2)): if matrix2[i][d] !=0: total += 1 df.append(total) #IDF idf = list() for i in df: tmp = 1 + log10(len(matrix2)/(1+i)) idf.append(tmp) tf = matrix2 tfidf = [] for baris in range(len(matrix2)): tampungBaris = [] for kolom in range(len(matrix2[0])): tmp = tf[baris][kolom] * idf[kolom] tampungBaris.append(tmp) tfidf.append(tampungBaris) write_csv(\"tfidf_%s.csv\"%n, tfidf) Feature Selection Seleksi fitur adalah salah satu tahapan praproses klasifikasi. Seleksi fitur dilakukan dengan cara memilih fitur-fitur yang relevan yang mempengaruhi hasil klasifikasi. Seleksi fitur digunakan untuk mengurangi dimensi data dan fitur-fitur yang tidak relevan. Seleksi fitur digunakan untuk meningkatkan efektifitas dan efisiensi kinerja dari algoritma klasifikasi. Kode Program Seleksi Fitur def pearsonCalculate(data, u,v): \"i, j is an index\" atas=0; bawah_kiri=0; bawah_kanan = 0 for k in range(len(data)): atas += (data[k,u] - meanFitur[u]) * (data[k,v] - meanFitur[v]) bawah_kiri += (data[k,u] - meanFitur[u])**2 bawah_kanan += (data[k,v] - meanFitur[v])**2 bawah_kiri = bawah_kiri ** 0.5 bawah_kanan = bawah_kanan ** 0.5 return atas/(bawah_kiri * bawah_kanan) def meanF(data): meanFitur=[] for i in range(len(data[0])): meanFitur.append(sum(data[:,i])/len(data)) return np.array(meanFitur) def seleksiFiturPearson(data, threshold, berhasil): global meanFitur data = np.array(data) meanFitur = meanF(data) u=0 while u < len(data[0]): dataBaru=data[:, :u+1] meanBaru=meanFitur[:u+1] seleksikata=berhasil[:u+1] v = u while v < len(data[0]): if u != v: value = pearsonCalculate(data, u,v) if value < threshold: dataBaru = np.hstack((dataBaru, data[:, v].reshape(data.shape[0],1))) meanBaru = np.hstack((meanBaru, meanFitur[v])) seleksikata = np.hstack((seleksikata, berhasil[v])) v+=1 data = dataBaru meanFitur=meanBaru berhasil=seleksikata if u%50 == 0 : print(\"proses : \", data.shape) u+=1 return data, seleksikata xBaru2,kataBaru = seleksiFiturPearson(tfidf, 0.9, berhasil) xBaru1,kataBaru2 = seleksiFiturPearson(xBaru2, 0.8, berhasil) write_csv(\"kata_pearson_%s.csv\"%n, kataBaru2) Clustering Clustering merupakan proses pengelompokan data dengan karakteristik yang sama ke suatu kelompok dan data dengan karakteristik berbeda ke kelompok yang lain. Metode yang digunakan dalam clustering ini yaitu menggunakan metode K- Mean dengan pendekatan Fuzzy. Setelah data di cluster, langkah berikutnya adalah menghitung nilai koefisien Silhouette. Kode Program Clustering print(\"Cluster dgn Seleksi Fitur : 0.8\") cntr, u, u0, distant, fObj, iterasi, fpc = fuzz.cmeans(xBaru1.T, 3, 2, 0.00001, 1000, seed=0) membership = np.argmax(u, axis=0) silhouette = silhouette_samples(xBaru1, membership) s_avg = silhouette_score(xBaru1, membership, random_state=10) for i in range(len(tfidf)): print(\"c \"+str(membership[i])) print(s_avg) References https://www.malasngoding.com/ http://www.teknologi-bigdata.com/2016/07/web-crawling-di-era-big-data.html https://yudiagusta.wordpress.com/clustering/ https://informatikalogi.com/text-preprocessing/amp/ https://devtrik.com/python/steeming-bahasa-indonesia-python-sastrawi/ https://informatikalogi.com/term-weighting-tf-idf/ https://wahyudisetiawan.wordpress.com/tag/seleksi-fitur/","title":"Home"},{"location":"#web-crawling","text":"Identitas : Nurul Vicky Wahdaniah 160411100128 Mata Kuliah Penambangan dan Pencarian Web","title":"WEB CRAWLING"},{"location":"#pendahuluan","text":"Halaman ini berisi tentang langkah - langkah crawling data dari sebuah website, kemudian dilanjutkan dengan pre-processing, seleksi fitur hingga clustering data.","title":"Pendahuluan"},{"location":"#tools-and-requirements","text":"Website URL : https://www.malasngoding.com/ Python 3.6 Python Libraries : BeautifulSoup4 Berfungsi untuk mengcrawl data pada website. Cara menginstall BeautifulSoup4 menggunakan pip : pip install beautifulsoup4 SQLite3 Berfungsi sebagai Database untuk penyimpanan data dari website. CSV Berfungsi untuk read and write tabular data dalam format CSV. Cara menginstall CSV module menggunakan pip : pip install python-csv Sastrawi Berfungsi untuk steeming pada bagian pre-processing data. Steeming bertujuan untuk mentransformasikan kata menjadi kata dasar tanpa imbuhan, baik awalan, akhiran atau sisipan. Casa menginstall Sastrawi menggunakan pip : pip install Sastrawi Math Berfungsi pada proses-proses yang menggunakan fungsi matematika. Cara menginstall modul Math menggunakan pip : pip install math Numpy Berfungsi untuk operasi vektor dan matriks untuk keperluan analisis data. Cara menginstall Numpy menggunakan pip : pip install numpy Sckit-Learn Berfungsi memberikan sejumlah fitur untuk keperluan data science seperti Algoritma Regresi, Algoritma Naive Bayes, Algoritma Clustering, Algoritma Decision Tree, Data Prepocessing Tool, dan lain - lain. Cara menginstall Sckit-Learn menggunakan pip : pip install scikit-learn Sckit-Fuzzy Merupakan kumpulan dari algoritma fuzzy yang digunakan pada modul SciPy atau SkLearn. Cara menginstall Sckit-Fuzzy menggunakan pip : pip install scikit-fuzzy Database KBI Digunakan pada proses pre-processing untuk memisahkan kata penting pada data.","title":"Tools and Requirements"},{"location":"#crawling","text":"Website URL : https://www.malasngoding.com/ Data yang diambil dari website di atas berupa data Text, yaitu Judul Artikel dan Isi Artikel. Langkah Proses Crawling : import library yang akan digunakan untuk crawl data. from bs4 import BeautifulSoup Request pada halaman web tertuju. src = \"https://www.malasngoding.com/\" page = requests.get(src) soup = BeautifulSoup(page.content, 'html.parser') simpan elemen/data (judul dan isi) yang akan di crawl pada variabel berdasar class pada tag html web tertuju. linkhead = soup.findAll(class_='text-dark') pagination = soup.find(class_='next page-numbers') code di atas merupakan variabel penampung untuk link yang menuju judul dan isi artikel konten = soup.find('article') title = konten.find(class_='post-title entry-title pb-2').getText() temp = konten.findAll('p') code di atas merupakan variabel yang menampung judul dan isi artikel Masukkan data ke dalam Database. conn = sqlite3.connect('articles.sqlite') conn.execute('''CREATE TABLE if not exists ARTICLES (TITLE TEXT NOT NULL, ISI TEXT NOT NULL);''') conn.execute(\"INSERT INTO ARTICLES (TITLE, ISI) VALUES (?, ?)\", (title, isif)); conn.commit() export data ke dalam format CSV ``` def write_csv(nama_file, isi, tipe='w'): 'tipe=w; write; tipe=a; append;' with open(nama_file, mode=tipe) as tbl: tbl_writer = csv.writer(tbl, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL) for row in isi: tbl_writer.writerow(row) ``` Kode Program Lengkap Proses Crawling : conn = sqlite3.connect('articles.sqlite') conn.execute('''CREATE TABLE if not exists ARTICLES (TITLE TEXT NOT NULL, ISI TEXT NOT NULL);''') conn.commit() src = \"https://www.malasngoding.com/\" n = 1 while n <= 2: print(\"page \",n) page = requests.get(src) soup = BeautifulSoup(page.content, 'html.parser') linkhead = soup.findAll(class_='text-dark') pagination = soup.find(class_='next page-numbers') for links in linkhead: try : src = links['href'] page = requests.get(src) soup = BeautifulSoup(page.content, 'html.parser') konten = soup.find('article') title = konten.find(class_='post-title entry-title pb-2').getText() temp = konten.findAll('p') isi = [] for j in range(len(temp)): isi += [temp[j].getText()] isif = \"\" for i in isi: isif += i conn.execute(\"INSERT INTO ARTICLES (TITLE, ISI) VALUES (?, ?)\", (title, isif)); except AttributeError: continue conn.commit() src = pagination['href'] n+=1 def write_csv(nama_file, isi, tipe='w'): 'tipe=w; write; tipe=a; append;' with open(nama_file, mode=tipe) as tbl: tbl_writer = csv.writer(tbl, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL) for row in isi: tbl_writer.writerow(row)","title":"Crawling"},{"location":"#pre-processing","text":"Pre-processing merupakan tahapan dimana program melakukan seleksi data yang akan di proses. Proses Pre-Processing meliputi : Case Folding Case Folding dibutuhkan dalam mengkonversi keseluruhan text dalam dokumen menjadi suatu bentuk standar/lower case. Lebih ringkasnya, case folding mengubah semua huruf dalam dokumen menjadi huruf kecil. hara huruf 'a' sampai 'z' yang diterima. Karakter selain huruf akan dihilangkan. Tokenizing Tahap tokenizing merupakan tahap pemecahan kalimat menjadi beberapa kata tunggal. Filtering (Stopword) Tahap filtering adalah tahap mengambil kata-kata penting dari hasil token. Stopword adalah kata-kata yang tidak deskriptif yang dapat dibuang dalam pendekatan bag-of-words. Stemming Sremming diperlukan untuk memperkecil jumlah indeks yang berbeda dari suatu dokumen, juga untuk melakukan pengelompokan kata-kata lain yang memiliki kata dasar dan arti yang serupa namun memiliki bentuk atau form yang berbeda karena mendapat imbuhan yang berbeda. Kode Program Pre-Processing cursor = conn.execute(\"SELECT* from ARTICLES\") isif ='' for row in cursor: isif+=row[1] factory = StopWordRemoverFactory() stopword = factory.create_stop_word_remover() from Sastrawi.Stemmer.StemmerFactory import StemmerFactory factory = StemmerFactory() stemmer = factory.create_stemmer() stop = stopword.remove(isif) stem = stemmer.stem(stop) katadasar = stem.split() matrix=[] for row in cursor: tampung = [] for i in katadasar: tampung.append(row[1].lower().count(i)) matrix.append(tampung) write_csv(\"kata_before_%s.csv\"%n, katadasar) conn = sqlite3.connect('KBI.db') cur_kbi = conn.execute(\"SELECT* from KATA\") def LinearSearch (kbi,kata): found=False posisi=0 while posisi < len (kata) and not found : if kata[posisi]==kbi: found=True posisi=posisi+1 return found berhasil=[] berhasil2='' for kata in cur_kbi : ketemu=LinearSearch(kata[0],katadasar) if ketemu : kata = kata[0] berhasil.append(kata) berhasil2=berhasil2+' '+kata","title":"Pre-Processing"},{"location":"#tf-idf","text":"TF (Term Frequency) TF (Term Frequency) adalah frekuensi dari kemunculan sebuah term dalam dokumen yang bersangkutan. Semakin besar jumlah kemunculan suatu term (TF tinggi) dalam dokumen, semakin besar pula bobotnya atau akan memberikan nilai kesesuaian yang semakin besar. IDF (Inverse Document Frequency) IDF (Inverse Document Frequency) merupakan sebuah perhitungan dari bagaimana term didistribusikan secara luas pada koleksi dokumen yang bersangkutan. IDF menunjukkan hubungan ketersediaan sebuah term dalam seluruh dokumen. Semakin sedikit jumlah dokumen yang mengandung term yang dimaksud, maka nilai IDF semakin besar. Kode Program TF-IDF #TF df = list() for d in range (len(matrix2[0])): total = 0 for i in range(len(matrix2)): if matrix2[i][d] !=0: total += 1 df.append(total) #IDF idf = list() for i in df: tmp = 1 + log10(len(matrix2)/(1+i)) idf.append(tmp) tf = matrix2 tfidf = [] for baris in range(len(matrix2)): tampungBaris = [] for kolom in range(len(matrix2[0])): tmp = tf[baris][kolom] * idf[kolom] tampungBaris.append(tmp) tfidf.append(tampungBaris) write_csv(\"tfidf_%s.csv\"%n, tfidf)","title":"TF - IDF"},{"location":"#feature-selection","text":"Seleksi fitur adalah salah satu tahapan praproses klasifikasi. Seleksi fitur dilakukan dengan cara memilih fitur-fitur yang relevan yang mempengaruhi hasil klasifikasi. Seleksi fitur digunakan untuk mengurangi dimensi data dan fitur-fitur yang tidak relevan. Seleksi fitur digunakan untuk meningkatkan efektifitas dan efisiensi kinerja dari algoritma klasifikasi. Kode Program Seleksi Fitur def pearsonCalculate(data, u,v): \"i, j is an index\" atas=0; bawah_kiri=0; bawah_kanan = 0 for k in range(len(data)): atas += (data[k,u] - meanFitur[u]) * (data[k,v] - meanFitur[v]) bawah_kiri += (data[k,u] - meanFitur[u])**2 bawah_kanan += (data[k,v] - meanFitur[v])**2 bawah_kiri = bawah_kiri ** 0.5 bawah_kanan = bawah_kanan ** 0.5 return atas/(bawah_kiri * bawah_kanan) def meanF(data): meanFitur=[] for i in range(len(data[0])): meanFitur.append(sum(data[:,i])/len(data)) return np.array(meanFitur) def seleksiFiturPearson(data, threshold, berhasil): global meanFitur data = np.array(data) meanFitur = meanF(data) u=0 while u < len(data[0]): dataBaru=data[:, :u+1] meanBaru=meanFitur[:u+1] seleksikata=berhasil[:u+1] v = u while v < len(data[0]): if u != v: value = pearsonCalculate(data, u,v) if value < threshold: dataBaru = np.hstack((dataBaru, data[:, v].reshape(data.shape[0],1))) meanBaru = np.hstack((meanBaru, meanFitur[v])) seleksikata = np.hstack((seleksikata, berhasil[v])) v+=1 data = dataBaru meanFitur=meanBaru berhasil=seleksikata if u%50 == 0 : print(\"proses : \", data.shape) u+=1 return data, seleksikata xBaru2,kataBaru = seleksiFiturPearson(tfidf, 0.9, berhasil) xBaru1,kataBaru2 = seleksiFiturPearson(xBaru2, 0.8, berhasil) write_csv(\"kata_pearson_%s.csv\"%n, kataBaru2)","title":"Feature Selection"},{"location":"#clustering","text":"Clustering merupakan proses pengelompokan data dengan karakteristik yang sama ke suatu kelompok dan data dengan karakteristik berbeda ke kelompok yang lain. Metode yang digunakan dalam clustering ini yaitu menggunakan metode K- Mean dengan pendekatan Fuzzy. Setelah data di cluster, langkah berikutnya adalah menghitung nilai koefisien Silhouette. Kode Program Clustering print(\"Cluster dgn Seleksi Fitur : 0.8\") cntr, u, u0, distant, fObj, iterasi, fpc = fuzz.cmeans(xBaru1.T, 3, 2, 0.00001, 1000, seed=0) membership = np.argmax(u, axis=0) silhouette = silhouette_samples(xBaru1, membership) s_avg = silhouette_score(xBaru1, membership, random_state=10) for i in range(len(tfidf)): print(\"c \"+str(membership[i])) print(s_avg)","title":"Clustering"},{"location":"#references","text":"https://www.malasngoding.com/ http://www.teknologi-bigdata.com/2016/07/web-crawling-di-era-big-data.html https://yudiagusta.wordpress.com/clustering/ https://informatikalogi.com/text-preprocessing/amp/ https://devtrik.com/python/steeming-bahasa-indonesia-python-sastrawi/ https://informatikalogi.com/term-weighting-tf-idf/ https://wahyudisetiawan.wordpress.com/tag/seleksi-fitur/","title":"References"}]}